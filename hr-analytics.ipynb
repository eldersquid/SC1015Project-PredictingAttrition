{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4207867,"sourceType":"datasetVersion","datasetId":2480666}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Basic Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt # we only need pyplot\nfrom pandas.plotting import scatter_matrix\n\nsb.set() # set the default Seaborn style for graphics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nhrdata = pd.read_csv('/kaggle/input/hr-analytics-prediction/HR-Employee-Attrition.csv')\nhrdata.head()\nprint(\"Data type : \", type(hrdata))\nprint(\"Data dims : \", hrdata.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T04:30:53.646803Z","iopub.execute_input":"2024-04-23T04:30:53.647203Z","iopub.status.idle":"2024-04-23T04:30:56.314017Z","shell.execute_reply.started":"2024-04-23T04:30:53.647172Z","shell.execute_reply":"2024-04-23T04:30:56.312807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first few rows of the DataFrame\nhrdata.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:30:56.315813Z","iopub.execute_input":"2024-04-23T04:30:56.316787Z","iopub.status.idle":"2024-04-23T04:30:56.347532Z","shell.execute_reply.started":"2024-04-23T04:30:56.316743Z","shell.execute_reply":"2024-04-23T04:30:56.346448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Data type : \", type(hrdata))\nprint(\"Data dims : \", hrdata.shape)\nhrdata.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:30:56.348828Z","iopub.execute_input":"2024-04-23T04:30:56.349199Z","iopub.status.idle":"2024-04-23T04:30:56.375171Z","shell.execute_reply.started":"2024-04-23T04:30:56.349171Z","shell.execute_reply":"2024-04-23T04:30:56.373269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"Delete NULL sums if any","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef check(hrdata):\n    result = []\n    for c in hrdata.columns:\n        dtype = hrdata[c].dtypes\n        instances = hrdata[c].count()\n        duplicates = hrdata[c].duplicated().sum()\n        unique = hrdata[c].nunique()\n        null = hrdata[c].isnull().sum()\n        result.append([dtype, instances, unique, null, duplicates])\n    data_check = pd.DataFrame(result, columns=[\"dtype\", \"instances\", \"unique\", \"null\", \"duplicates\"], index=hrdata.columns)\n    return data_check\n\ncheck(hrdata)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:30:56.378146Z","iopub.execute_input":"2024-04-23T04:30:56.381065Z","iopub.status.idle":"2024-04-23T04:30:56.424013Z","shell.execute_reply.started":"2024-04-23T04:30:56.381021Z","shell.execute_reply":"2024-04-23T04:30:56.422940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hrdata.describe().T","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:30:56.425389Z","iopub.execute_input":"2024-04-23T04:30:56.425862Z","iopub.status.idle":"2024-04-23T04:30:56.514779Z","shell.execute_reply.started":"2024-04-23T04:30:56.425821Z","shell.execute_reply":"2024-04-23T04:30:56.513993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CatVar = hrdata.select_dtypes(include=['object'])\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 10))\n\nfor i, col in enumerate(CatVar):\n    row_id = int(i / 3)\n    col_id = int(i % 3)\n    sb.countplot(data=hrdata, x=col, color='#FF6347', ax=axes[row_id, col_id])\n    axes[row_id, col_id].set_xlabel('')\n    axes[row_id, col_id].set_title(col, fontsize=12)\n\ntotal = len(hrdata)\nfor rows in axes:\n    for ax in rows:\n        ax.set_yticklabels([])\n        ax.set_ylabel('')\n        for p in ax.patches:\n            percentage = f'{100 * p.get_height() / total:.1f}%\\n'\n            x = p.get_x() + p.get_width() / 2\n            y = p.get_height()\n            ax.annotate(percentage, (x, y), ha='center', va='center', fontsize=7)\n\n\naxes[1, 0].tick_params(axis='x', rotation=20)\naxes[1, 2].tick_params(axis='x', rotation=65)\n\nsb.despine(left=True, )\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:30:56.516128Z","iopub.execute_input":"2024-04-23T04:30:56.516659Z","iopub.status.idle":"2024-04-23T04:30:58.792761Z","shell.execute_reply.started":"2024-04-23T04:30:56.516628Z","shell.execute_reply":"2024-04-23T04:30:58.791847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the bar graph shown, given that the 'Over18' variable has only one distinct value, it is redundant and can be safely omitted. The data reveals that the majority of employees are engaged in Research & Development, with infrequent travel and minimal overtime. Notably, there's a significant disproportion between attrition and non-attrition cases, necessitating resampling before training the machine learning model to address this imbalance.\n","metadata":{}},{"cell_type":"code","source":"NumVar = hrdata.select_dtypes(include=['int64'])\n\nhrdata[NumVar.columns].hist(color='#1E90FF', figsize=(20, 10))\nsb.despine(left=True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:30:58.793933Z","iopub.execute_input":"2024-04-23T04:30:58.795172Z","iopub.status.idle":"2024-04-23T04:31:06.123923Z","shell.execute_reply.started":"2024-04-23T04:30:58.795114Z","shell.execute_reply":"2024-04-23T04:31:06.122820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorical variables with int64 data type:\n* education\n* environment satisfaction\n* job involvement \n* job level\n* job satisfaction\n* performance rating\n* RelationshipSatisfaction\n* stockoptionlevel\n* trainingtimeslastyear\n* Worklifebalance\n\nRedundant Variables (Has only 1-2 unique values, making it redundant for analysis and can be removed):\n* PerformanceRating\n* StandardHours\n* EmployeeCount\n","metadata":{}},{"cell_type":"code","source":"duplicated_columns = hrdata.columns[hrdata.columns.duplicated()]\nif len(duplicated_columns) > 0:\n    print(\"Duplicated columns:\", duplicated_columns)\n    duplicated_columns_df = df[duplicated_columns]\n    print(\"Data for duplicated columns:\")\n    print(duplicated_columns_df)\n\nelse:\n    print(\"No duplicated columns found.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:06.125335Z","iopub.execute_input":"2024-04-23T04:31:06.125758Z","iopub.status.idle":"2024-04-23T04:31:06.132898Z","shell.execute_reply.started":"2024-04-23T04:31:06.125729Z","shell.execute_reply":"2024-04-23T04:31:06.131787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = NumVar.corr()\n\n# Create a heatmap\nplt.figure(figsize=(20, 15))\nsb.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, square=True,\n            linewidths=.5, cbar_kws={\"shrink\": .5}, annot_kws={\"fontsize\": 7})\nplt.title('Correlation Matrix of Numerical Variables', fontsize=16)\nplt.yticks(rotation=0)  # Ensure y-axis labels are not rotated\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:06.134339Z","iopub.execute_input":"2024-04-23T04:31:06.134710Z","iopub.status.idle":"2024-04-23T04:31:09.012391Z","shell.execute_reply.started":"2024-04-23T04:31:06.134680Z","shell.execute_reply":"2024-04-23T04:31:09.011229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation analysis indicates a clear, strong positive relationship between 'TotalWorkingYears' and both 'JobLevel' and 'MonthlyIncome'. Additionally, 'YearsInCurrentRole' shows a strong positive correlation with 'YearsWithCurrentManager' and 'YearsAtCompany'. Therefore, we will delve deeper into these attributes and their interrelationships.","metadata":{}},{"cell_type":"code","source":"attri1 = ['Age', 'MonthlyIncome','JobLevel','TotalWorkingYears']\nscatter_matrix(hrdata[attri1], color='cornflowerblue', figsize=(18, 12));\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:09.017622Z","iopub.execute_input":"2024-04-23T04:31:09.018049Z","iopub.status.idle":"2024-04-23T04:31:11.018230Z","shell.execute_reply.started":"2024-04-23T04:31:09.018014Z","shell.execute_reply":"2024-04-23T04:31:11.017124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.scatter(hrdata['JobLevel'], hrdata['MonthlyIncome'], color='cornflowerblue')\nplt.xlabel('Job Level')\nplt.ylabel('Monthly Income')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(hrdata['TotalWorkingYears'], hrdata['MonthlyIncome'], color='cornflowerblue')\nplt.xlabel('Total Working Years')\nplt.ylabel('Monthly Income')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(hrdata['Age'], hrdata['TotalWorkingYears'], color='cornflowerblue')\nplt.xlabel('Age')\nplt.ylabel('Total Working Years')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(hrdata['JobLevel'],hrdata['Age'], color='cornflowerblue')\nplt.ylabel('Age')\nplt.xlabel('Job Level')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:50:44.511199Z","iopub.execute_input":"2024-04-23T04:50:44.511656Z","iopub.status.idle":"2024-04-23T04:50:46.089753Z","shell.execute_reply.started":"2024-04-23T04:50:44.511623Z","shell.execute_reply":"2024-04-23T04:50:46.088551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Based on the scatter matrix, here are a few observations:**\n\nMonthly Income vs Job Level: There appears to be a positive correlation between monthly income and job level. This means that people with higher job levels tend to have higher monthly incomes.\n\nMonthly Income vs Total Working Years: The scatter plot suggests a possible positive correlation between monthly income and total working years. People with more working experience may tend to have higher incomes.\n\nAge vs Total Working Years: As expected, there appears to be a positive correlation between age and total working years. People tend to accumulate more working years as they age.\n\nJob Level vs Age: There seems to be a positive correlation between job level and age. This could be because people tend to gain experience and move into higher job levels as they age.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nplt.scatter(hrdata['YearsWithCurrManager'], hrdata['YearsInCurrentRole'], color='cornflowerblue')\nplt.xlabel('Years with Current Manager')\nplt.ylabel('Years in Current Role')\nplt.title('Years with Current Manager vs. Years in Current Role', fontsize=14)\nplt.show()\n\nplt.figure(figsize=(6, 6))\nplt.scatter(hrdata['YearsWithCurrManager'], hrdata['YearsAtCompany'], color='cornflowerblue')\nplt.xlabel('Years with Current Manager')\nplt.ylabel('Years at Company')\nplt.title('Years with Current Manager vs. Years at Company', fontsize=14)\nplt.show()\n\nplt.figure(figsize=(6, 6))\nplt.scatter(hrdata['YearsInCurrentRole'], hrdata['YearsAtCompany'], color='cornflowerblue')\nplt.xlabel('Years in Current Role')\nplt.ylabel('Years at Company')\nplt.title('Years in Current Role vs. Years at Company', fontsize=14)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T05:27:34.364548Z","iopub.execute_input":"2024-04-23T05:27:34.365017Z","iopub.status.idle":"2024-04-23T05:27:35.653064Z","shell.execute_reply.started":"2024-04-23T05:27:34.364985Z","shell.execute_reply":"2024-04-23T05:27:35.651957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attri2 = ['YearsAtCompany', 'YearsInCurrentRole', 'YearsWithCurrManager']\nscatter_matrix(hrdata[attri2], color='cornflowerblue', figsize=(18, 12));","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:14.132376Z","iopub.execute_input":"2024-04-23T04:31:14.133557Z","iopub.status.idle":"2024-04-23T04:31:16.124979Z","shell.execute_reply.started":"2024-04-23T04:31:14.133514Z","shell.execute_reply":"2024-04-23T04:31:16.123855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Based on the scatter matrix, here are a few observations:**\n\nYears with Current Manager vs. Years in Current Role: There appears to be a weak positive correlation between the number of years with the current manager and the number of years in the current role. This means that people who have been with their current manager for a longer time tend to have also been in their current role for a longer time. This is likely due to people staying with a manager they like.\n\nYears with Current Manager vs. Years at Company: The scatter plot suggests a weak positive correlation between the number of years with the current manager and the number of years at the company. People who have been with their manager for a longer time also tend to have been at the company longer. This could be because they are happy with their manager and the company, or it could be due to other factors such as industry norms or lack of mobility in their field.\n\nYears in Current Role vs. Years at Company: The data shows a positive correlation between the number of years in the current role and the number of years at the company. People who have been in their current role for a longer time tend to have also been at the company longer. This is likely because people who stay in a role for a long time are likely to stay with the company as well.","metadata":{}},{"cell_type":"code","source":"\n\ncategorical_columns = hrdata.select_dtypes(include='object').columns\n\n# Determine grid size based on the number of columns (adjust as needed)\ngrid_width = 3  # Adjust based on desired number of columns per row\ngrid_height = (len(categorical_columns) // grid_width) + (len(categorical_columns) % grid_width > 0)\n\n# Configure plot size based on grid dimensions\nplt.figure(figsize=(grid_width * 8, grid_height * 5))\n\n# Loop through each categorical column\nfor i, col in enumerate(categorical_columns):\n    # Create a subplot for each column\n    plt.subplot(grid_height, grid_width, i + 1)\n\n    # Create a countplot with Seaborn\n    ax = sb.countplot(x=col, data=hrdata)  # Get the underlying axes object\n\n    # Access container objects (bars) created by Seaborn\n    containers = ax.containers\n\n    # Calculate total non-null values\n    total = len(hrdata[col].dropna())\n\n    # Loop through container objects and add annotations\n    for c in containers:\n        patches = c.patches  # Access patches within each container\n        for p in patches:\n            height = p.get_height()\n            plt.text(p.get_x() + p.get_width() / 2,\n                     height + 0.1,  # Adjust vertical position for better visibility\n                     '{:.1f}%'.format((height / total) * 100),\n                     ha='center', va='bottom')\n\n    # Customize plot for readability\n    plt.title(f'Distribution of {col}')\n    plt.xlabel(col)\n    plt.ylabel('Count')\n    ax.set_xlabel('')  # Remove x-axis label using the current axis object\n    if i == 3 or i== 5:  # Index for the fifth plot (i starts from 0)\n        plt.xticks(rotation=45)\n\n\n# Adjust layout and display plots\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:16.126514Z","iopub.execute_input":"2024-04-23T04:31:16.127788Z","iopub.status.idle":"2024-04-23T04:31:18.601514Z","shell.execute_reply.started":"2024-04-23T04:31:16.127728Z","shell.execute_reply":"2024-04-23T04:31:18.600699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Set the overall style\nsb.set(style=\"whitegrid\")\n\n# Get categorical columns\ncategorical_columns = hrdata.select_dtypes(include='object').columns\n# Remove 'Attrition' from categorical_columns\ncategorical_columns = categorical_columns.drop('Attrition')\n\n\n# Determine grid size based on whether the number of columns is even or odd\nif len(categorical_columns) % 2 == 0:\n    grid_size = (len(categorical_columns) // 2, 2)\nelse:\n    grid_size = (len(categorical_columns) // 2 + 1, 2)\n\nfig, axes = plt.subplots(*grid_size, figsize=(20,23))\n\n# Flatten axes for easy iteration\naxes = axes.flatten()\npalette = \"Paired\"\n\n# Loop through categorical columns and create countplots\nfor i, column in enumerate(categorical_columns):\n    ax = sb.countplot(x=column, hue='Attrition', data=hrdata, palette=palette, ax=axes[i])\n    \n    # Set title and labels\n    ax.set_title(f\"Distribution of {column} by Attrition\", fontsize=16)\n    ax.set_xlabel(column, fontsize=14)\n    ax.set_ylabel('Count', fontsize=14)\n    ax.legend(title='Attrition', loc='upper right')\n    axes[i].set_xlabel('')\n    \n    total = len(hrdata[column])\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() / 2.,\n                height + 3,\n                '{:.1f}%'.format((height / total) * 100),\n                ha=\"center\")\n    \n\naxes[4].tick_params(axis='x', rotation=65)\n    \n# Remove any unused axes\nfor j in range(i+1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout\nfig.tight_layout()\n\n# Show the plots\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:18.602723Z","iopub.execute_input":"2024-04-23T04:31:18.603271Z","iopub.status.idle":"2024-04-23T04:31:21.582756Z","shell.execute_reply.started":"2024-04-23T04:31:18.603241Z","shell.execute_reply":"2024-04-23T04:31:21.581682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the distribution graphs above, we can see that employees who travel less (Non-Travel 9.4% or Travel_Rarely 60.3%) have a significantly lower attrition rate than those who travel frequently (Travel_Frequently 4.7%). Travel_Frequently has the highest attrition rate (4.7%) among all categories in BusinessTravel. Frequent travel might be causing stress or work-life balance issues, leading to higher attrition rates among employees who travel frequently. It also suggests that employees who don't travel may be more satisfied with their work-life balance, potentially preferring Work From Home (WFH).\n\nFor Department, attrition is the lowest in Research & Development (9.0%), followed by Sales (6.3%) and Human Resources (0.8%). Despite having the lowest attrition rate, Research & Development also has the highest percentage of employees who do not leave the company (56.3%).\n\nIn EducationField, employees with a Life Sciences (35.2% or 6.1%) or Medical (27.3% or 4.3%) background make up the majority of the workforce, and both groups have low attrition.\n\nIn the gender column, male employees tend to have higher attrition rates (10.2%) as compared to female employees (5.9%). But the attrition rate is not that big, so we can assume there are no gender discrimination.\n\nSome insights that we see from Job Role is that Research Scientists (3.2%) and Laboratory Technicians (4.2%) show higher attrition rates compared to other job roles, while Research Directors (0.1%) and Managers (0.3%) have the lowest attrition rates. Sales Representatives have the highest attrition rate (2.2%) among all Sales-related job roles.\n\nWe also discovered that single employees have the highest attrition rate (8.2%), while married (5.7%) and divorced (2.2%) employees have relatively lower attrition which could be influenced by various factors such as work-life balance, job flexibility, or career growth opportunities. \n\nFor Overtime column, we realized that employees who work overtime have a slightly higher attrition rate (8.6%) compared to those who do not work overtime (7.5%). The slightly higher attrition rate among employees who work overtime suggests that long working hours could contribute to employee dissatisfaction.\n\nThis analysis shows that JobRole, Overtime, and BusinessTravel might be important fields to take note of.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Set the overall style\nsb.set(style=\"whitegrid\")\n\n# Get numerical columns\nnumerical_columns = hrdata.select_dtypes(include=['int64']).columns\n\n# Drop categorical variables with int64 data type\ncolumns_to_drop = [\n    'Education', 'EnvironmentSatisfaction', 'JobInvolvement',\n    'JobLevel', 'JobSatisfaction', 'PerformanceRating',\n    'RelationshipSatisfaction', 'StockOptionLevel',\n    'TrainingTimesLastYear', 'WorkLifeBalance'\n]\nnumerical_columns = numerical_columns.drop(columns_to_drop)\n\n# Determine grid size\ngrid_size = (4, len(numerical_columns) // 4 + (1 if len(numerical_columns) % 4 != 0 else 0))\n\nfig, axes = plt.subplots(*grid_size, figsize=(20, 15))\n\n# Flatten axes for easy iteration\naxes = axes.flatten()\npalette = \"Set2\"\n\n# Loop through numerical columns and create boxplots\nfor i, column in enumerate(numerical_columns):\n    ax = sb.boxplot(x='Attrition', y=column, data=hrdata, palette=palette, ax=axes[i])\n    \n    # Set title and labels\n    ax.set_title(f\"{column} by Attrition\", fontsize=16)\n    ax.set_xlabel('Attrition', fontsize=14)\n    ax.set_ylabel(column, fontsize=14)\n    ax.set_xticklabels(['No Attrition', 'Attrition'])\n    axes[i].set_xlabel('')\n\n# Remove any unused axes\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout\nfig.tight_layout()\n\n# Show the plots\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:21.584275Z","iopub.execute_input":"2024-04-23T04:31:21.585135Z","iopub.status.idle":"2024-04-23T04:31:25.640875Z","shell.execute_reply.started":"2024-04-23T04:31:21.585103Z","shell.execute_reply":"2024-04-23T04:31:25.639821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the lack of variation observed in the box plots, we have decided to drop the following attributes from our analysis:\n- EmployeeCount\n- HourlyRate\n- EmployeeNumber\n- MonthlyRate\n- StandardHours\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Dropping Redundant and Unnecessary Columns","metadata":{}},{"cell_type":"markdown","source":"Dropping the categorical variables with int64 data type","metadata":{}},{"cell_type":"code","source":"# Create another DataFrame containing only numerical varaibles \nint64_df = hrdata.select_dtypes(include=['int64'])\nint64_df.drop(['JobInvolvement','Education','RelationshipSatisfaction','JobSatisfaction','JobLevel','EnvironmentSatisfaction','PerformanceRating','StockOptionLevel','TrainingTimesLastYear','WorkLifeBalance'],axis=1,inplace=True)\n\n# Print information about the new DataFrame\nprint(int64_df.info())","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:25.642367Z","iopub.execute_input":"2024-04-23T04:31:25.642752Z","iopub.status.idle":"2024-04-23T04:31:25.658784Z","shell.execute_reply.started":"2024-04-23T04:31:25.642718Z","shell.execute_reply":"2024-04-23T04:31:25.657731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In total, the following attributes are being removed from consideration due to their limited variability, relevance, and having only 1-2 unique values:\n- PerformanceRating\n- StandardHours\n- EmployeeCount\n- Over18\n- EmployeeNumber\n- HourlyRate\n- MonthlyRate\n\nBy dropping attributes with limited variability, we have streamlined our analysis to focus on key factors that offer meaningful insights into employee behaviour. This optimization process enhances the predictive power and interpretability of our models, enabling us to develop more accurate retention strategies to address attrition challenges effectively.\n","metadata":{}},{"cell_type":"code","source":"# Dropping unnecessary columns\n\ncolumns_drop1 = ['PerformanceRating',\n'StandardHours',\n'EmployeeCount',\n'Over18',\n'EmployeeNumber',\n'HourlyRate',\n'MonthlyRate'] \n\ncleaned_dropped_hrdata1 = hrdata.drop(columns_drop1, axis=1)\n\ncleaned_dropped_hrdata1.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:25.660378Z","iopub.execute_input":"2024-04-23T04:31:25.660692Z","iopub.status.idle":"2024-04-23T04:31:25.730062Z","shell.execute_reply.started":"2024-04-23T04:31:25.660653Z","shell.execute_reply":"2024-04-23T04:31:25.728961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(cleaned_dropped_hrdata1, test_size=0.2, random_state=123, stratify=hrdata.Attrition)\nprint('Shapes:', train_df.shape, test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:25.731412Z","iopub.execute_input":"2024-04-23T04:31:25.731823Z","iopub.status.idle":"2024-04-23T04:31:25.990165Z","shell.execute_reply.started":"2024-04-23T04:31:25.731796Z","shell.execute_reply":"2024-04-23T04:31:25.988827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_df['Attrition'].copy() #response\nx_train = train_df.drop(columns='Attrition') #predictor\n\ny_test = test_df['Attrition'].copy()  # Response\nx_test = test_df.drop(columns='Attrition')  # Predictor\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:25.992752Z","iopub.execute_input":"2024-04-23T04:31:25.993820Z","iopub.status.idle":"2024-04-23T04:31:26.001876Z","shell.execute_reply.started":"2024-04-23T04:31:25.993773Z","shell.execute_reply":"2024-04-23T04:31:26.000661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oversampling is not done to the test dataframe so as to not cause data leakage, and to preserve the real life attrition distribution proportions.","metadata":{}},{"cell_type":"code","source":"# Dropping unnecessary columns\n\ncolumns_drop2 = ['PerformanceRating',\n'StandardHours',\n'EmployeeCount',\n'Over18',\n'EmployeeNumber',\n'HourlyRate',\n'MonthlyRate','Attrition'] \n#drop attrition as well because target variable we are trying to predict should not be included in predictors\n\ncleaned_dropped_hrdata2 = hrdata.drop(columns_drop2, axis=1)\n\ncleaned_dropped_hrdata2.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.003223Z","iopub.execute_input":"2024-04-23T04:31:26.003621Z","iopub.status.idle":"2024-04-23T04:31:26.076092Z","shell.execute_reply.started":"2024-04-23T04:31:26.003583Z","shell.execute_reply":"2024-04-23T04:31:26.074951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NumVar = train_df.select_dtypes(include=['int64'])\nCatVar = train_df.select_dtypes(include=['object'])\n\n#both cat and num variables do not include attribute\ncategorical_attrs = list(set(CatVar.columns) - set(columns_drop2))\nnumerical_attrs = list(set(NumVar.columns) - set(columns_drop2))\n\nprint(\"Categorical Attributes:\")\nprint(categorical_attrs)\n\nprint(\"\\nNumerical Attributes:\")\nprint(numerical_attrs)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.077549Z","iopub.execute_input":"2024-04-23T04:31:26.077937Z","iopub.status.idle":"2024-04-23T04:31:26.086735Z","shell.execute_reply.started":"2024-04-23T04:31:26.077896Z","shell.execute_reply":"2024-04-23T04:31:26.085583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Note this does not include attribute because x_train consists of the predictors\n#x_train = x_train[categorical_attrs+numerical_attrs].copy()\n#x_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.088252Z","iopub.execute_input":"2024-04-23T04:31:26.088593Z","iopub.status.idle":"2024-04-23T04:31:26.096384Z","shell.execute_reply.started":"2024-04-23T04:31:26.088565Z","shell.execute_reply":"2024-04-23T04:31:26.095195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Engineering**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.097711Z","iopub.execute_input":"2024-04-23T04:31:26.098427Z","iopub.status.idle":"2024-04-23T04:31:26.115889Z","shell.execute_reply.started":"2024-04-23T04:31:26.098383Z","shell.execute_reply":"2024-04-23T04:31:26.114712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to transform categorical labels into numerical values.\nencoder = LabelEncoder()\n#Transforms y_train and y_test into numerical labels, replaces the original categorical values with integers\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.transform(test_df['Attrition'])\n\nassert y_train.sum() == train_df[train_df.Attrition=='Yes'].shape[0] #used to check if count match","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.117743Z","iopub.execute_input":"2024-04-23T04:31:26.118216Z","iopub.status.idle":"2024-04-23T04:31:26.128157Z","shell.execute_reply.started":"2024-04-23T04:31:26.118166Z","shell.execute_reply":"2024-04-23T04:31:26.126993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The purpose of this pipeline is to preprocess and transform the \n#features in the x_train data before feeding it into a machine learning model\n\n# Create a transformation pipeline\ncustom_transformer = ColumnTransformer([\n    ('numerical', StandardScaler(), numerical_attrs),\n    ('categorical', OneHotEncoder(), categorical_attrs),\n], remainder='drop')\n\n# Apply the transformation to the training data\nx_train = custom_transformer.fit_transform(x_train)\n\n# Apply the transformation to the test data\nx_test = custom_transformer.transform(x_test)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.130200Z","iopub.execute_input":"2024-04-23T04:31:26.130634Z","iopub.status.idle":"2024-04-23T04:31:26.165757Z","shell.execute_reply.started":"2024-04-23T04:31:26.130565Z","shell.execute_reply":"2024-04-23T04:31:26.164825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resampling\n\nAs mentioned above in the EDA, resampling is required because there are many more cases of non-attrition compared to attrition to balance the dataset. Imbalanced datasets can lead machine learning algorithms to perform poorly, as the model tends to be biased towards the majority. As there are 1470 employees in the dataset, it is a small number. Therefore, we have to OVER-SAMPLE so that we can avoid any data lost. The dataset will then bloat the attrition cases as a result.\n\nWe decided to use **SMOTE** (Synthetic Minority Over-sampling Technique) as our oversampling method. SMOTE works by creating synthetic samples from the minority class by selecting similar instances and interpolating between them.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=123)\nx_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.167511Z","iopub.execute_input":"2024-04-23T04:31:26.167871Z","iopub.status.idle":"2024-04-23T04:31:26.594632Z","shell.execute_reply.started":"2024-04-23T04:31:26.167842Z","shell.execute_reply":"2024-04-23T04:31:26.592748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = []\n\nfor i in custom_transformer.transformers_:\n    if hasattr(i[1], 'get_feature_names_out'):\n        names = i[1].get_feature_names_out(i[2])  \n    else:\n        names = [f\"{i[0]}_{j}\" for j in i[2]] \n    feature_names.extend(names)\n\n# Create a DataFrame with the transformed x_train and feature names\ntransformed_df = pd.DataFrame(data=x_train_smote, columns=feature_names)\n\n# Print the transformed DataFrame\ntransformed_df","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.596183Z","iopub.execute_input":"2024-04-23T04:31:26.597083Z","iopub.status.idle":"2024-04-23T04:31:26.659154Z","shell.execute_reply.started":"2024-04-23T04:31:26.597042Z","shell.execute_reply":"2024-04-23T04:31:26.657850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_test_df = pd.DataFrame(data=x_test, columns=feature_names)\ntransformed_test_df","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.668642Z","iopub.execute_input":"2024-04-23T04:31:26.671535Z","iopub.status.idle":"2024-04-23T04:31:26.720825Z","shell.execute_reply.started":"2024-04-23T04:31:26.671480Z","shell.execute_reply":"2024-04-23T04:31:26.719942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom tabulate import tabulate","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.722058Z","iopub.execute_input":"2024-04-23T04:31:26.722362Z","iopub.status.idle":"2024-04-23T04:31:26.752992Z","shell.execute_reply.started":"2024-04-23T04:31:26.722337Z","shell.execute_reply":"2024-04-23T04:31:26.752087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_results(model, x_train_smote, y_train_smote, x_test, y_test):\n    # Cross-validated predictions\n    y_train_pred_cv = cross_val_predict(model, x_train_smote, y_train_smote, cv=10)\n    \n    # Training the model on full training set\n    model.fit(x_train_smote, y_train_smote)\n    \n    # Predictions on test set\n    y_test_pred = model.predict(x_test)\n    \n    # Metrics for training set\n    train_confusion = confusion_matrix(y_train_smote, y_train_pred_cv)\n    train_accuracy = accuracy_score(y_train_smote, y_train_pred_cv)\n    train_precision = precision_score(y_train_smote, y_train_pred_cv)\n    train_recall = recall_score(y_train_smote, y_train_pred_cv)\n    train_f1 = f1_score(y_train_smote, y_train_pred_cv)\n    train_classification = classification_report(y_train_smote, y_train_pred_cv, output_dict=True)\n    \n    # Metrics for test set\n    test_confusion = confusion_matrix(y_test, y_test_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    test_precision = precision_score(y_test, y_test_pred)\n    test_recall = recall_score(y_test, y_test_pred)\n    test_f1 = f1_score(y_test, y_test_pred)\n    test_classification = classification_report(y_test, y_test_pred, output_dict=True)\n    \n    # Creating DataFrames for metrics\n    metrics = {\n        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n        'Training Set': [train_accuracy, train_precision, train_recall, train_f1],\n        'Test Set': [test_accuracy, test_precision, test_recall, test_f1]\n    }\n    \n    df_metrics = pd.DataFrame(metrics)\n    \n    # Creating DataFrames for classification reports\n    df_train_classification = pd.DataFrame(train_classification).transpose()\n    df_test_classification = pd.DataFrame(test_classification).transpose()\n    \n    # Creating the nicely formatted table\n    table_metrics = tabulate(df_metrics, headers='keys', tablefmt='pretty', showindex=False)\n    table_train_classification = tabulate(df_train_classification, headers='keys', tablefmt='pretty')\n    table_test_classification = tabulate(df_test_classification, headers='keys', tablefmt='pretty')\n    \n    # Printing the tables\n    print('Model:', model.__class__.__name__)\n    \n    print('\\nMetrics:')\n    print(table_metrics)\n    \n    print('\\nTraining Set (Cross-validated):')\n    print('Confusion matrix:\\n', train_confusion)\n    print('\\nClassification Report:')\n    print(table_train_classification)\n    \n    print('\\nTest Set:')\n    print('Confusion matrix:\\n', test_confusion)\n    print('\\nClassification Report:')\n    print(table_test_classification)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.754417Z","iopub.execute_input":"2024-04-23T04:31:26.754743Z","iopub.status.idle":"2024-04-23T04:31:26.768835Z","shell.execute_reply.started":"2024-04-23T04:31:26.754715Z","shell.execute_reply":"2024-04-23T04:31:26.767886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_reg = LogisticRegression(random_state=123,max_iter=1000)\ndisplay_results(log_reg, x_train_smote, y_train_smote, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:26.770246Z","iopub.execute_input":"2024-04-23T04:31:26.770877Z","iopub.status.idle":"2024-04-23T04:31:27.725115Z","shell.execute_reply.started":"2024-04-23T04:31:26.770833Z","shell.execute_reply":"2024-04-23T04:31:27.723556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(random_state=123)\ndisplay_results(dtc, x_train_smote, y_train_smote, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:27.727145Z","iopub.execute_input":"2024-04-23T04:31:27.728410Z","iopub.status.idle":"2024-04-23T04:31:28.111025Z","shell.execute_reply.started":"2024-04-23T04:31:27.728353Z","shell.execute_reply":"2024-04-23T04:31:28.109859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state=123)\ndisplay_results(rf, x_train_smote, y_train_smote, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:28.112488Z","iopub.execute_input":"2024-04-23T04:31:28.113197Z","iopub.status.idle":"2024-04-23T04:31:33.577337Z","shell.execute_reply.started":"2024-04-23T04:31:28.113163Z","shell.execute_reply":"2024-04-23T04:31:33.575996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Which is the best?\n\nUsing the results shown above for each machine learning algorithm with its metrics displayed, there are a number of ways where we can decide the final machine learning algorithm to use. We can use accuracy, precision, f1-score, and recall categories. However, since we are trying to predict attrition, we should focus more on True Positives as these people are correctly predicted to leave the company based on the variables, whilst also making False Negatives as little as possible. So, the category that we must pay CAREFUL attention to is the RECALL category which is a measure of correctly identify actual positives in the dataset.","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression\n<table>\n  <tr>\n    <th>Metric</th>\n    <th>Training Set</th>\n    <th>Test Set</th>\n  </tr>\n  <tr>\n    <td>Accuracy</td>\n    <td>0.8801020408163265</td>\n    <td>0.8673469387755102</td>\n  </tr>\n  <tr>\n    <td>Precision</td>\n    <td>0.7094017094017094</td>\n    <td>0.6666666666666666</td>\n  </tr>\n  <tr>\n    <td>Recall</td>\n    <td>0.4368421052631579</td>\n    <td>0.3404255319148936</td>\n  </tr>\n  <tr>\n    <td>F1-score</td>\n    <td>0.5407166123778502</td>\n    <td>0.4507042253521127</td>\n  </tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"Decision Tree Classifier\n<table>\n  <tr>\n    <th>Metric</th>\n    <th>Training Set</th>\n    <th>Test Set</th>\n  </tr>\n  <tr>\n    <td>Accuracy</td>\n    <td>0.7814625850340136</td>\n    <td>0.7517006802721088</td>\n  </tr>\n  <tr>\n    <td>Precision</td>\n    <td>0.3562231759656652</td>\n    <td>0.3088235294117647</td>\n  </tr>\n  <tr>\n    <td>Recall</td>\n    <td>0.4368421052631579</td>\n    <td>0.44680851063829785</td>\n  </tr>\n  <tr>\n    <td>F1-score</td>\n    <td>0.39243498817966904</td>\n    <td>0.3652173913043478</td>\n  </tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"Random Forest Classifier\n<table>\n  <tr>\n    <th>Metric</th>\n    <th>Training Set</th>\n    <th>Test Set</th>\n  </tr>\n  <tr>\n    <td>Accuracy</td>\n    <td>0.8545918367346939</td>\n    <td>0.8639455782312925</td>\n  </tr>\n  <tr>\n    <td>Precision</td>\n    <td>0.7111111111111111</td>\n    <td>0.8181818181818182</td>\n  </tr>\n  <tr>\n    <td>Recall</td>\n    <td>0.16842105263157894</td>\n    <td>0.19148936170212766</td>\n  </tr>\n  <tr>\n    <td>F1-score</td>\n    <td>0.2723404255319149</td>\n    <td>0.31034482758620685</td>\n  </tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"From the 3 algorithms, we can see that the Decision Tree Classifier has the best recall rate for both training and test sets at 0.4368 and 0.4468 respectively. So we may say that Decision Tree Classifier is the best model to use. However, its precision is 0.3562 and 0.3088 for training and test sets respectively. This implies that there will be many False Positives(employees that are expected to leave do not leave in the end). So in the end, if precision is also important, then using f1-score is IDEAL because it is the harmonic mean between Recall and Precision.\n\nLooking at all 3 algorithms, **Logistic Regression** is the best to use as its f1-score is higher than the others, at 0.5407 and 0.4507 for training and test set respectively.","metadata":{}},{"cell_type":"markdown","source":"# Feature Importance\n\nNow, we will train a logistic regression model and then evaluate the importance of each feature in predicting attrition. By examining the coefficients assigned to each feature, it identifies which features have the greatest impact on predicting attrition. This is crucial for understanding which factors contribute most significantly to attrition within the dataset.\n\n","metadata":{}},{"cell_type":"code","source":"log_reg.fit(x_train, y_train)\nfeature_importance = log_reg.coef_[0]\nfeature_importance_df = pd.DataFrame({'Feature': transformed_df.columns, 'Importance': feature_importance})\n# Sort by value\nfeature_importance_df = feature_importance_df.reindex(feature_importance_df.Importance.abs().sort_values(ascending=False).index)\nprint(feature_importance_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:33.578700Z","iopub.execute_input":"2024-04-23T04:31:33.579560Z","iopub.status.idle":"2024-04-23T04:31:33.718862Z","shell.execute_reply.started":"2024-04-23T04:31:33.579527Z","shell.execute_reply":"2024-04-23T04:31:33.717460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 12))\nplt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Variables\")\nplt.gca().invert_yaxis()  # Invert the y-axis to show the features with higher importance at the top\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:33.720857Z","iopub.execute_input":"2024-04-23T04:31:33.723115Z","iopub.status.idle":"2024-04-23T04:31:34.615750Z","shell.execute_reply.started":"2024-04-23T04:31:33.723058Z","shell.execute_reply":"2024-04-23T04:31:34.614550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph shown, we should focus on the ABSOLUTE value of the feature importance as it is how you rank the importance. The largest absolute coefficients will be the most important features. So, we can see that Overtime, Business Travel, and Years since Last Promotion are prominent variables to look out for.\n\n\nOverTime: Both \"OverTime_No\" and \"OverTime_Yes\" are highly ranked in terms of absolute importance, indicating that whether an employee works overtime or not is a significant factor.\n\nBusinessTravel: All three categories of BusinessTravel (\"Non-Travel,\" \"Travel_Frequently,\" and \"Travel_Rarely\") are important. This suggests that how often an employee travels for business is a significant predictor.\n\nYearsSinceLastPromotion: This variable is also highly ranked, indicating that the number of years since the last promotion is an important factor.","metadata":{}},{"cell_type":"code","source":"\n# Create a DataFrame to store feature names and their importances\nfeature_importance_df = pd.DataFrame({'Feature': transformed_df.columns, 'Importance': feature_importance})\n\n# Sort by absolute value of importance\nfeature_importance_df = feature_importance_df.reindex(feature_importance_df.Importance.abs().sort_values(ascending=False).index)\n\n# Separate positive and negative features\npositive_features = feature_importance_df[feature_importance_df['Importance'] > 0].head(5)  # Top 5 positive\nnegative_features = feature_importance_df[feature_importance_df['Importance'] < 0].head(5)  # Top 5 negative\n\n# Concatenate positive and negative features\ntop_features = pd.concat([positive_features, negative_features])\n\n# Print the top 5 positive and top 5 negative features\nprint(\"Top 5 Positive Features:\")\nprint(positive_features)\nprint(\"\\nTop 5 Negative Features:\")\nprint(negative_features)\n\n# Plot the feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Variables\")\nplt.gca().invert_yaxis()  # Invert the y-axis to show the features with higher importance at the top\nplt.title(\"Top 5 Positive and Top 5 Negative Feature Importances\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:34.617365Z","iopub.execute_input":"2024-04-23T04:31:34.618004Z","iopub.status.idle":"2024-04-23T04:31:35.110324Z","shell.execute_reply.started":"2024-04-23T04:31:34.617963Z","shell.execute_reply":"2024-04-23T04:31:35.109167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Top 5 Positive Features:**\n\nOverTime_Yes (Importance: 0.934879):\nEmployees who work overtime (OverTime_Yes) have a higher probability of being predicted as leaving the company.\nThis suggests that overtime work might be a significant factor contributing to attrition.\n\nJobRole_Sales Representative (Importance: 0.739612):\nEmployees in the Sales Representative role have a higher probability of being predicted as leaving.\nThis indicates that employees in this role might have higher attrition rates compared to other roles.\n\nBusinessTravel_Travel_Frequently (Importance: 0.729629):\nEmployees who travel frequently for business have a higher likelihood of being predicted as leaving.\nFrequent business travel could potentially lead to job dissatisfaction or work-life balance issues.\n\nYearsSinceLastPromotion (Importance: 0.608708):\nThe number of years since the last promotion has a positive impact on the prediction of attrition.\nEmployees who have not been promoted for a long time are more likely to leave the company.\n\nJobRole_Laboratory Technician (Importance: 0.600676):\nEmployees in the Laboratory Technician role are more likely to be predicted as leaving.\nThis role might have specific challenges or factors that contribute to higher attrition rates.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Top 5 Negative Features:**\n\nOverTime_No (Importance: -0.935323):\nEmployees who do not work overtime (OverTime_No) have a higher probability of being predicted as staying.\nThis contrasts with the positive importance of 'OverTime_Yes', indicating that not working overtime is associated with lower attrition.\n\nBusinessTravel_Non-Travel (Importance: -0.848187):\nEmployees who do not travel for business (BusinessTravel_Non-Travel) are more likely to be predicted as staying.\nNon-traveling employees have a lower likelihood of leaving the company.\n\nJobRole_Healthcare Representative (Importance: -0.665387):\nHealthcare Representatives have a lower probability of being predicted as leaving.\nThis role seems to have lower attrition compared to other roles.\n\nYearsInCurrentRole (Importance: -0.539116):\nThe number of years in the current role has a negative impact on the prediction of attrition.\nEmployees who have been in their current role for a longer time are less likely to leave.\n\nYearsWithCurrManager (Importance: -0.485755):\nThe number of years with the current manager also has a negative impact on attrition prediction.\nLonger tenure with the current manager suggests a lower likelihood of leaving.","metadata":{}},{"cell_type":"code","source":"\n# Sort by absolute value\nfeature_importance_df = feature_importance_df.reindex(feature_importance_df.Importance.abs().sort_values(ascending=False).index)\n\n# Create separate DataFrames for Job Roles, Departments, and Education Fields\njob_roles_df = feature_importance_df[feature_importance_df['Feature'].str.startswith('JobRole_')]\ndepartments_df = feature_importance_df[feature_importance_df['Feature'].str.startswith('Department_')]\neducation_fields_df = feature_importance_df[feature_importance_df['Feature'].str.startswith('EducationField_')]\n\n# Create tables for Job Roles, Departments, and Education Fields\njob_roles_table = job_roles_df.rename(columns={'Feature': 'Job Role', 'Importance': 'Attrition Coefficient'})\ndepartments_table = departments_df.rename(columns={'Feature': 'Department', 'Importance': 'Attrition Coefficient'})\neducation_fields_table = education_fields_df.rename(columns={'Feature': 'Education Field', 'Importance': 'Attrition Coefficient'})\n\n# Print the tables\nprint(\"Job Roles with Attrition Coefficients:\")\nprint(job_roles_table)\nprint(\"\\nDepartments with Attrition Coefficients:\")\nprint(departments_table)\nprint(\"\\nEducation Fields with Attrition Coefficients:\")\nprint(education_fields_table)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:35.111878Z","iopub.execute_input":"2024-04-23T04:31:35.112261Z","iopub.status.idle":"2024-04-23T04:31:35.132961Z","shell.execute_reply.started":"2024-04-23T04:31:35.112227Z","shell.execute_reply":"2024-04-23T04:31:35.131531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the tables by 'Attrition Coefficient' in ascending order\njob_roles_table_sorted = job_roles_table.sort_values('Attrition Coefficient')\ndepartments_table_sorted = departments_table.sort_values('Attrition Coefficient')\neducation_fields_table_sorted = education_fields_table.sort_values('Attrition Coefficient')\n\n# Create plots for Job Roles, Departments, and Education Fields in ascending order\nplt.figure(figsize=(12, 8))\nplt.subplot(3, 1, 1)\nplt.barh(job_roles_table_sorted['Job Role'], job_roles_table_sorted['Attrition Coefficient'], color='skyblue')\nplt.xlabel(\"Attrition Coefficient\")\nplt.ylabel(\"Job Role\")\nplt.title(\"Attrition Coefficients by Job Role (Ascending)\")\n\nplt.subplot(3, 1, 2)\nplt.barh(departments_table_sorted['Department'], departments_table_sorted['Attrition Coefficient'], color='lightgreen')\nplt.xlabel(\"Attrition Coefficient\")\nplt.ylabel(\"Department\")\nplt.title(\"Attrition Coefficients by Department (Ascending)\")\n\nplt.subplot(3, 1, 3)\nplt.barh(education_fields_table_sorted['Education Field'], education_fields_table_sorted['Attrition Coefficient'], color='salmon')\nplt.xlabel(\"Attrition Coefficient\")\nplt.ylabel(\"Education Field\")\nplt.title(\"Attrition Coefficients by Education Field (Ascending)\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:35.134412Z","iopub.execute_input":"2024-04-23T04:31:35.135438Z","iopub.status.idle":"2024-04-23T04:31:36.363961Z","shell.execute_reply.started":"2024-04-23T04:31:35.135400Z","shell.execute_reply":"2024-04-23T04:31:36.363018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find indices of test data where attrition is positive\nnp.where(y_test==1)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:36.365221Z","iopub.execute_input":"2024-04-23T04:31:36.365843Z","iopub.status.idle":"2024-04-23T04:31:36.375002Z","shell.execute_reply.started":"2024-04-23T04:31:36.365809Z","shell.execute_reply":"2024-04-23T04:31:36.373527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use trained model to predict probabilities on test data\nemployee_index = 97  # Index of the employee you want to predict for\nemployee_features = [x_test[employee_index]]  \nattrition_probability = log_reg.predict_proba(employee_features)[0, 1] * 100\n\n\n# Print information about the employee\nprint(\"Information of employee:\\n\")\nprint(test_df.iloc[employee_index])\n\n# Print probability of attrition\nprint(f'\\n\\n\\nThe employee has {attrition_probability:.1f}% chances of attrition.')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:36.376688Z","iopub.execute_input":"2024-04-23T04:31:36.377980Z","iopub.status.idle":"2024-04-23T04:31:36.390705Z","shell.execute_reply.started":"2024-04-23T04:31:36.377939Z","shell.execute_reply":"2024-04-23T04:31:36.389457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use trained model to predict probabilities on test data\nemployee_index = 285  # Index of the employee you want to predict for\nemployee_features = [x_test[employee_index]]  \nattrition_probability = log_reg.predict_proba(employee_features)[0, 1] * 100\n\n# Print information about the employee\nprint(\"Information of employee:\\n\")\nprint(test_df.iloc[employee_index])\n\n# Print probability of attrition\nprint(f'\\n\\n\\nThe employee has {attrition_probability:.1f}% chances of attrition.')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:31:36.393027Z","iopub.execute_input":"2024-04-23T04:31:36.393642Z","iopub.status.idle":"2024-04-23T04:31:36.404179Z","shell.execute_reply.started":"2024-04-23T04:31:36.393537Z","shell.execute_reply":"2024-04-23T04:31:36.403142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Overall Analysis:**\n\nOvertime: Working overtime (OverTime_Yes) is a significant factor contributing to attrition, while not working overtime (OverTime_No) is associated with lower attrition.\n\nBusiness Travel: Frequent business travel (BusinessTravel_Travel_Frequently) is linked to higher attrition, while non-traveling employees (BusinessTravel_Non-Travel) are more likely to stay.\n\nJob Roles: Specific job roles like Sales Representatives and Laboratory Technicians show higher attrition, while Healthcare Representatives have lower attrition.\n\nPromotion and Tenure: Longer periods since the last promotion and longer tenure in current roles or with current managers are associated with higher employee retention.\n\nDepartments: Sales department shows a higher likelihood of attrition, while Research & Development and Human Resources have lower attrition coefficients.\n\nEducation Fields: Employees with Technical Degrees and Human Resources education tend to have higher attrition, while those in Life Sciences, Medical, and Other fields have lower attrition coefficients.","metadata":{}},{"cell_type":"markdown","source":"## **Conclusion**\nIn conclusion, this analysis provides valuable insights into the factors influencing employee attrition within the organization. Leveraging HR analytics and machine learning, we have identified key features that contribute significantly to attrition, both positively and negatively.\n\nThe top positive features, such as Overtime_Yes, BusinessTravel_Travel_Frequently, YearsSinceLastPromotion, and specific job roles like Sales Representatives and Laboratory Technicians, indicate areas where proactive retention strategies can be implemented. These features suggest that employees working overtime, traveling frequently for business, or who have not been promoted recently are more likely to consider leaving. Addressing these factors with targeted interventions can improve employee retention and job satisfaction.\n\nConversely, negative features like lack of overtime (OverTime_No), non-business travel (BusinessTravel_Non-Travel), and longer tenure in roles or with managers (YearsInCurrentRole, YearsWithCurrManager) also play a crucial role in attrition prediction. Employees who do not work overtime, do not travel for business, or have long tenures in their current roles or with their managers are less likely to leave. Understanding these factors allows HR to tailor strategies to retain valuable talent and mitigate turnover risks.","metadata":{}},{"cell_type":"markdown","source":"**Below are recommendations aiming to improve work-life balance, career advancement opportunities, and managerial support to enhance employee satisfaction and retention:**\n\nAddress Overtime Issues: Consider evaluating workload and overtime policies to reduce stress and improve work-life balance.\n\nBusiness Travel Policies: Review business travel requirements and their impact on employees' job satisfaction and retention.\n\nJob Role Analysis: Investigate the specific challenges or reasons behind high attrition rates in Sales Representative and Laboratory Technician roles and develop targeted retention programs.\n\nPromotion and Career Growth: Implement programs to provide regular promotions and career growth opportunities to reduce attrition related to stagnation.\n\nManagerial Support: Recognize the importance of supportive managers and encourage longer tenure with the same manager to improve retention.","metadata":{}}]}